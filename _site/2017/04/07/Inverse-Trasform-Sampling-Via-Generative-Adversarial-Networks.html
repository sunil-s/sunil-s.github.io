
<h2 id="inverse-transform-sampling-via-generative-adversarial-networks">Inverse Transform Sampling Via Generative Adversarial Networks</h2>

<p><strong>What is Inverse Transform Sampling?</strong></p>

<p><a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a> (ITS) is a generic technique used for generating independent sample numbers at random given any underlying probability distribution. It is akin to other random number generation techniques such as <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>, <a href="https://en.wikipedia.org/wiki/Ziggurat_algorithm">Ziggurat algorithm</a> and <a href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box-Muller transform</a>. It operates as follows:
suppose we wish to generate samples from a continuous probability distribution with cumulative density function (CDF) $F_X(x)$. ITS takes a uniform sample of a random variable $U$ that is disributed between $[0,1]$, interprets it as a probability, and generates sample $x$ for which $F_X(x) = u$, i.e., $x = F^{-1}_X(u)$.</p>

<p>As an illustration, consider the case when we wish to generate samples from an exponential distribution, i.e., $F_X(x)=(1-e^{-\lambda x})$ for $x\geq 0$ and $0$ otherwise. Using ITS, we obtain $F_X(x) = u$, or equivalently, $x=-\frac{1}{\lambda}\ln(1-u)$. The mapping between $u$ and $x$ is indicated in the following graph.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">rcParams</span>
<span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'font.size'</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>

<span class="n">l</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># lambda</span>
<span class="n">u_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">u_range</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u_range</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'u'</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Exponential distribution with mean $1$'</span><span class="p">)</span>

<span class="c"># Sample value of u</span>
<span class="n">u</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">u</span><span class="p">,</span><span class="n">u</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">)],</span><span class="s">'r--'</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="n">u</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">)],</span><span class="s">'r--'</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">u</span><span class="p">),</span><span class="s">'rs'</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="/assets/images/2017-04-07-GANs/ITS.png" alt="png" /></p>

<p>In the above depiction, each of the randomly generated numbers from a uniform distribution are mapped to a sample $x$. For instance, when $\lambda=1$, $u=0.75$ is mapped to $x\approx1.38$ (marked by the red dashed lines).</p>

<p>Conceptually, the ITS method makes sense since the CDF of any random variable lies between $0$ and $1$, and samples $x$ are generated according to their contribution towards the CDF. This approach also extends to discrete distributions, if we simply generate the largest sample $x$ from the domain of the distribution such that $F_X(x)\leq u$.</p>

<p>The above method will, however, not work in several cases such as:</p>
<ul>
  <li>Unknown distribution (when the distribution of the random variable is not known in tractable form)</li>
  <li>Diverging or analytically intractable CDF, e.g. <a href="https://en.wikipedia.org/wiki/Hyperbolic_distribution">Hyperbolic distribution</a></li>
  <li>Non-invertible or analytically intractable CDF, e.g., <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a>, <a href="https://en.wikipedia.org/wiki/Cauchy_distribution">Cauchy distribution</a></li>
  <li>Mixture of densities, e.g., <a href="https://en.wikipedia.org/wiki/Mixture_distribution">Mixture Distribution</a></li>
  <li>Functions or transformations of random variables, e.g. for generating square of a Gaussian distribution</li>
  <li>Multi-dimensional random variables, e.g., <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">Multivariate Gaussian distribution</a></li>
</ul>

<p>Moreover, in order to obtain a large number of samples, one needs to perform the CDF inversion multiple times, which is expensive. One possible way to reduce the number of inversions to only a few while obtaining a large number of samples is the application of the so-called the <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2529691"><em>Stochastic Collocation Monte Carlo</em> (SCMC) sampler</a>. However, SCMS methods are computationally expensive as they use the squared Bessel process.</p>

<blockquote>
  <p>An immediate follow-up question becomes “Are there other methods using which we can generate independent random numbers with a lower computation overhead? Say hello to GANs.</p>
</blockquote>

<p><strong>What are GANs?</strong></p>

<p>Generative Adversarial Networks (GANs) are examples of generative models in the machine learning literature. They are used in the context of unsupervised or semi-supervised learning. Using a few training data samples, GANs learn to represent an estimate of the distribution. For a detailed introduction to GANs, please refer to the seminal paper by Goodfellow et al. <a href="https://arxiv.org/abs/1406.2661">[1]</a> or the same author’s excellent summary on GANs at NIPS 2016 <a href="https://arxiv.org/abs/1701.00160">[2]</a>.</p>

<p>GANs essentially comprise two deep neural networks, the <strong>Discriminator</strong> and the <strong>Generator</strong>. The discriminator function $D$ takes in an input sample $x$ and determines the probability $D(x)$, of $x$ having been drawn from the original data distribution. The generator function $G$ takes in a noisy sample $z$, and generates a sample $G(z)$, that is intended to fool the discriminator into believing that the sample is indeed from the original data distribution. While the generator attempts to generate samples that are probabilistically close to the training data, the discriminator attempts to deem the generated samples as fake, i.e., not having been drawn from the true distribution. A conceptual illustration of a GAN generating a MNIST digit sample is provided below (Picture credit <a href="https://www.slideshare.net/ckmarkohchang/generative-adversarial-networks">[3]</a>).
<img src="/assets/images/2017-04-07-GANs/GAN_concept.png" alt="GANs concept" /></p>

<p><strong><em>The objective function</em></strong></p>

<p>Together, the discriminator and the generator compete against each other in a <em>zero-sum game</em> (wherein one player’s gain is the other’s loss). Given a set of training data samples $x$ and noise samples $z$, the gain (loss) function for the discriminator (generator) respectively, is given by</p>

<p>$J(D,G)=\mathbb{E}<em>{x\sim data}\log_e D(x) + \mathbb{E}</em>{z\sim noise}\log_e \left(1-D(G(z))\right)$.</p>

<p>Given parameters $\theta_D$ and $\theta_G$ for the discriminator and generator respectively, GANs play the minimax game: $\min_{\theta_G}\max_{\theta_D}J(\theta_D,\theta_G)$.</p>

<p>One immediate benefit from using GANs over ITS is that GANs can generate data with only some data samples and without knowing anything about the underlying distribution. Thus, they work well even with unweildy, functions of random variables or multidimensional distributions. At the same time, however, training GANs requires finding the <em>Nash equilibrium</em> (we will define this shortly) of a game, which maybe more time-consuming than inverting a CDF.</p>

<p>The framework for GANs (picture courtesy <a href="https://arxiv.org/abs/1701.00160">[2]</a>) is depicted below. The discriminator $D$ has to maximize $J(D,G)$ with respect to parameters $\theta_D$. Since the logarithm is a monotonic function, $D$ tries to make $D(x)$ close to $1$ and $D(G(z))$ close to $0$. On the other hand, the generator $G$ has to minimize $J(D,G)$ with respect to parameters $\theta_G$; so $G$ attempts to make $D(G(z))$ close to $1$. In other words, in this mini-max game, the discriminator wants to deem the generated samples as fake, while the generator wants to fool the discriminator into believing that the generated samples are real. In the example below involving human faces, the human face on the left is a real image, while the discriminator is led into believing that the image on the right is real as well.</p>

<p><img src="/assets/images/2017-04-07-GANs/GAN_model.png" alt="Adversarial Networks Framework." /></p>

<p>By competing or co-operating against each other, the system attains a <em>Nash equilibrium</em> corresponding to the generator sample distribution converging to the data distribution. The Nash equilibrium is defined as the solution concept of a game between two players wherein each player has chosen a strategy and neither player can benefit by changing strategies while the other players keep theirs unchanged.</p>

<p>Let us do some math now concerning the Nash equilibrium. Recall the loss function for GAN is given by</p>

<script type="math/tex; mode=display">\begin{aligned}
 J(D,G) = \mathbb{E}_{x\sim data}\log_e D(x) + \mathbb{E}_{z\sim noise}\log_e \left(1-D(G(z))\right) \\
\end{aligned}</script>

<p>When $G$ is fixed (which happens when a Nash equilibrium is attained; and then we call G’s output as the <em>model</em>), we obtain</p>

<script type="math/tex; mode=display">\begin{aligned} 
 J(D) = \mathbb{E}_{x\sim data}\log_e D(x) + \mathbb{E}_{x\sim model}\log_e \left(1-D(x)\right)
\end{aligned}</script>

<p>Assuming that both $p_{model}$ and $p_{data}$ have the same sample space or basis, we can write
<script type="math/tex">% <![CDATA[
\begin{aligned}
 J(D) &= \int_x\left[p_{data}(x)\log_e D(x) + p_{model}(x)\log_e \left(1-D(x)\right)\right] \text{dx} \\ 
\end{aligned} %]]></script></p>

<p>This is maximized when $\frac{\partial J}{\partial D} = 0$, i.e.,
<script type="math/tex">\begin{aligned}
\frac{p_{data}(x)}{D(x)} = \frac{p_{model}(x)}{1-D(x)}
\end{aligned}</script>
In other words, for fixed $G$, the optimal discriminator is given by
<script type="math/tex">\begin{aligned}
D(x) = \frac{p_{data}(x)}{p_{data}(x)+p_{model}(x)}
\end{aligned}</script></p>

<p>At convergence, $G$ fixates to the model and  $p_{data}(x)\sim p_{model}(x)$. When this happens, the discriminator’s output is always equal to $\frac{1}{2}$, i.e., $D(x)=\frac{1}{2}\forall x$; with this, $J(D,G)$ reaches a global minima of $-\log_e(4)\approx -1.386$. This number is useful while checking if the training for a GAN has converged.</p>

<p><strong><em>Training</em></strong></p>

<p>In the following, we use GANs to generate samples for certain toy distributions. For this (rather simple) use case, both the discriminator and generator networks were taken to be multi layer perceptrons (MLPs). A general architecture of the GAN network based on MLPs looks like the figure below (image credit <a href="http://people.ee.duke.edu/~lcarin/Yunchen9.30.2016.pdf">[4]</a>.
<img src="/assets/images/2017-04-07-GANs/GAN_network.PNG" alt="Adversarial Network." /></p>

<p>There are a couple of tips and tricks we need to keep in mind while designing GANs:</p>

<ul>
  <li>
    <p>It is very important to balance the discriminator and generator; and usually, the discriminator ``wins’’. In other words, it is important to ensure that the discriminator is bigger and deeper than the generator, as otherwise it will not have sufficient capacity to learn to be able to distinguish accurately between generated and real samples. For our experiments, we used a generator with a MLP two hidden layers with $32$ nodes each, and the ReLu non-linearity between the layers, while the discriminator was a three-layered MLP with $32$ nodes each, and the ReLu non-linearity between layers. The last layer of the discriminator is taken to be a sigmoid, since the output can then be directly interpreted as a probability.</p>
  </li>
  <li>
    <p>It is sometimes beneficial to train $D$ for a longer duration than $G$.The original GAN algorithm from <a href="https://arxiv.org/abs/1406.2661">[1]</a> is pasted below. Notice that the algorithm interleaves $k$ steps of discriminator training for every generator training step. $k$ is a hyper-parameter; for our basic experiments here, we used $k=1$ like in the original paper.
<img src="/assets/images/2017-04-07-GANs/algorithm.PNG" alt="GAN algorithm" /></p>
  </li>
</ul>

<p><strong><em>Preliminary Results</em></strong></p>

<h5 id="noise-distribution">Noise distribution</h5>
<p>For all our simulations, we take the noise distribution $p_z(Z)$ to be a Gaussian with zero mean and unit standard variance, i.e., $z\sim\mathcal{N}(0,1)$. The noise histogram is shown below.
<img src="/assets/images/2017-04-07-GANs/noise.PNG" alt="noise" /></p>

<h5 id="case-1-generating-a-normal-distribution-with-a-different-mean-and-variance">Case 1: Generating a Normal distribution with a different mean and variance</h5>
<p>First, we attempt to generate samples for another Gaussian distribution with a mean of $5$ and a variance of $2$. We train the GAN for $5000$ iterations each for $D$ and $G$ (or a total of $10000$ steps); however, the loss is seen to converge to its expected stationary value of $\approx -1.38$ around $1000$ steps itself.
<img src="/assets/images/2017-04-07-GANs/1dGaussian.PNG" alt="1d Gaussian" /></p>

<h5 id="case-2-generating-a-bi-modal-normal-distribution">Case 2: Generating a bi-modal Normal distribution</h5>
<p>Second, we attempt to generate samples for a bi-modal Gaussian distribution. The bi-modal distribution is obtained as a linear combination of two Gaussian distributions, $\mathcal{N}(-3,2)$ and $\mathcal{N}(3,1)$.
<img src="/assets/images/2017-04-07-GANs/1dBimodalGaussian.PNG" alt="1d Gaussian" />
We see that the generated samples’ histogram matches the true data distribution reasonably well. We did not tweak the network parameters here further to potentially improve performance.</p>

<p><strong><em>Closing Notes</em></strong></p>

<p>In this blog, we have provided an introduction to how Generative Adversarial Networks can overcome some of the shortcomings of Inverse Transform Sampling methods. We have also illustrated via a couple of fundamental examples that GANs can emulate Gaussian distribution mixtures very effectively with basic neural network architectures. The ideas presented here can be extended (without much deliberation) to generate multi-dimensional distributions that represent more tangible data such as images. GANs of late are able to generate high compleling high-resolution examples using convolutional neural networks from diverse image classes. The GAN we introduced in this blog is the barebones or vanilla GAN. This model has been improved upon by several variants of GAN such as Deep Convolutional GANs, Conditional GANs, Autoencoding GANs and Infomation Maximizing GANs <a href="http://people.ee.duke.edu/~lcarin/Yunchen9.30.2016.pdf">[4]</a></p>

<p><strong><em>References</em></strong></p>

<p>[1] I. Goodfellow et. al, <a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></p>

<p>[2] I. Goodfellow, <a href="https://arxiv.org/abs/1701.00160">NIPS 2016 Tutorial: Generative Adversarial Networks</a></p>

<p>[3] M. Chang, <a href="https://www.slideshare.net/ckmarkohchang/generative-adversarial-networks">GAN slides</a></p>

<p>[4] Y. Pu, <a href="http://people.ee.duke.edu/~lcarin/Yunchen9.30.2016.pdf">GAN and its variants</a></p>
