{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# True distribution\n",
    "def get_data_samples(N,type=0):\n",
    "    \n",
    "    if type == 0:\n",
    "        # Gaussian\n",
    "        mu = 5\n",
    "        sigma = 2\n",
    "        samples = np.random.normal(mu, sigma, N)\n",
    "        samples = samples.reshape(-1,1)\n",
    "    \n",
    "    if type == 1:\n",
    "        # Mixture of Gaussians\n",
    "        mu1 = 3\n",
    "        sigma1 = 1\n",
    "        mu2 = -3\n",
    "        sigma2 = 2\n",
    "        select = np.random.binomial(1,0.5,N)\n",
    "        samples = select*np.random.normal(mu1, sigma1, N) + (1-select)*np.random.normal(mu2, sigma2, N)\n",
    "        samples = samples.reshape(-1,1)\n",
    "\n",
    "    if type == 2:\n",
    "        # Two dimensional distribution\n",
    "        means = np.array([0,0])\n",
    "        cov = np.array([[2,1],[1,2]])\n",
    "        samples = np.random.multivariate_normal(means, cov, N)\n",
    "        samples = samples.reshape(-1,2)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_samples = get_data_samples(100000,2)\n",
    "data_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data_samples.shape[1] == 2:\n",
    "    plt.hexbin(data_samples[:,0],data_samples[:,1])\n",
    "    plt.colorbar()\n",
    "else:\n",
    "    plt.hist(data_samples,100)\n",
    "    xlims = 10\n",
    "    plt.xlim(-xlims,xlims)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Noise for generator input\n",
    "def get_noise_samples(N):\n",
    "#     low = 0\n",
    "#     high = 1\n",
    "#     samples = np.random.uniform(low, high, N)\n",
    "    mu = 0\n",
    "    sigma = 1\n",
    "    samples = np.random.normal(mu, sigma, N)\n",
    "    samples = samples.reshape(-1,1)\n",
    "    return samples\n",
    "\n",
    "noise_samples = get_noise_samples(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(noise_samples,100)\n",
    "xlims = 5\n",
    "plt.xlim(-xlims,xlims)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generator/Discriminator network\n",
    " \n",
    "def Network(inp, hidden_sizes, I,O):\n",
    "    weights = {}\n",
    "    biases = {}\n",
    "    inp_dim = I\n",
    "\n",
    "    for i in xrange(len(hidden_sizes)):\n",
    "        op_dim = hidden_sizes[i]\n",
    "        weights[i] = tf.get_variable(name=\"w\" + `i`,shape=[inp_dim, op_dim],\n",
    "                                     initializer=tf.random_normal_initializer(0, 0.1))\n",
    "        biases[i] = tf.get_variable(name=\"b\" + `i`,shape=(op_dim,),initializer=tf.constant_initializer(0))\n",
    "\n",
    "        tf.summary.histogram(\"w\" + `i`,weights[i])\n",
    "        tf.summary.histogram(\"b\"+`i`,biases[i])\n",
    "        \n",
    "        op = tf.nn.relu(tf.matmul(inp,weights[i]) + biases[i])\n",
    "        inp = op\n",
    "        inp_dim = op_dim\n",
    "\n",
    "    op_dim = O\n",
    "    weights[i+1] = tf.get_variable(name=\"w\" + `i+1`,shape=(inp_dim, op_dim),\n",
    "                                   initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    biases[i+1] = tf.get_variable(name=\"b\" + `i+1`,shape=(op_dim,),initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    tf.summary.histogram(\"w\"+`i+1`,weights[i+1])\n",
    "    tf.summary.histogram(\"b\"+`i+1`,biases[i+1])\n",
    "    \n",
    "    op = tf.matmul(inp,weights[i+1]) + biases[i+1]\n",
    "    tf.summary.histogram(\"op\",op)  \n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function to convert to a probability\n",
    "def sigmoid(x):\n",
    "    return 1./(1+tf.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_hidden_sizes = [50,50]\n",
    "D_hidden_sizes = [50,50,50] # D uses a larger network since it should be able to discriminate\n",
    "lr = 0.001\n",
    "\n",
    "# Clear graph if it already exists\n",
    "tf.reset_default_graph()\n",
    "\n",
    "GANgraph = tf.Graph()\n",
    "\n",
    "with GANgraph.as_default():\n",
    "    data = tf.placeholder(tf.float32,shape=[None,None])\n",
    "    noise = tf.placeholder(tf.float32,shape=[None,None])\n",
    "    \n",
    "    with tf.variable_scope(\"D\"):\n",
    "        I = 2\n",
    "        O = 1\n",
    "        D = sigmoid(Network(data,D_hidden_sizes,I,O))\n",
    "    with tf.variable_scope(\"G\"):\n",
    "        I = 1\n",
    "        O = 2\n",
    "        G = Network(noise,G_hidden_sizes,I,O)\n",
    "\n",
    "    with tf.variable_scope(\"D\",reuse=True):\n",
    "        I = 2\n",
    "        O = 1\n",
    "        DG = sigmoid(Network(G,D_hidden_sizes,I,O))\n",
    "\n",
    "    # Collecting variables since we need to use SGD separately\n",
    "    D_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='D')  \n",
    "    G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='G')\n",
    "    \n",
    "    # Also, as per the paper’s suggestion, it’s better to maximize tf.reduce_mean(tf.log(D_fake))\n",
    "    # instead of minimizing tf.reduce_mean(1 - tf.log(D_fake)).\n",
    "    lossD = -tf.reduce_mean(tf.log(D)) + tf.reduce_mean(tf.log(1-DG))\n",
    "    lossG = -tf.reduce_mean(tf.log(DG))\n",
    "    tf.summary.scalar(\"LossD\",lossD)\n",
    "    tf.summary.scalar(\"LossG\",lossG)\n",
    "    \n",
    "    SGDoptimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    Doptimizer = SGDoptimizer.minimize(lossD,var_list=D_vars)\n",
    "    Goptimizer = SGDoptimizer.minimize(lossG,var_list=G_vars)\n",
    "    \n",
    "    data_mean = tf.reduce_mean(data)\n",
    "    G_mean = tf.reduce_mean(G)\n",
    "    \n",
    "    # KL_divergence = tf.contrib.distributions.kl(data_dist,G_dist)\n",
    "    mean_divergence = data_mean-G_mean\n",
    "    tf.summary.scalar(\"mean_divergence\",mean_divergence)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph=GANgraph)\n",
    "LOGDIR = './logdir'\n",
    "writer = tf.summary.FileWriter(LOGDIR,GANgraph)\n",
    "\n",
    "k = 1\n",
    "n_iter = 10000\n",
    "batch_size = 1000\n",
    "type = 1\n",
    "\n",
    "with sess:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    step = 0\n",
    "    \n",
    "    for iter in xrange(n_iter):                \n",
    "        for idx in xrange(k):\n",
    "            \n",
    "            _, summary, _, _ = sess.run([Doptimizer, summary_op, lossD, lossG], \n",
    "                                feed_dict={data:get_data_samples(batch_size,type=type), noise:get_noise_samples(batch_size)})\n",
    "        \n",
    "            writer.add_summary(summary,global_step=step)\n",
    "            step += 1\n",
    "\n",
    "        _, md, summary = sess.run([Goptimizer, mean_divergence, summary_op], \n",
    "                                feed_dict={data:get_data_samples(batch_size,type=type), noise:get_noise_samples(batch_size)})\n",
    "        writer.add_summary(summary,global_step=step)\n",
    "        step += 1\n",
    "        \n",
    "    # Generating new samples\n",
    "    N = 100000    \n",
    "    generated_samples = sess.run(G, feed_dict={noise:get_noise_samples(N)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if data_samples.shape[1] == 2:\n",
    "    plt.hexbin(generated_samples[:,0],generated_samples[:,1])\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(generated_samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting histogram of the generated samples and comparing with original\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "num_bins = 100\n",
    "y,x = np.histogram(get_data_samples(N,type),num_bins)\n",
    "x = x[:-1] + (x[1] - x[0])/2 \n",
    "plt.plot(x,y,linewidth=3)\n",
    "\n",
    "y,x = np.histogram(get_noise_samples(N),num_bins)\n",
    "x = x[:-1] + (x[1] - x[0])/2 \n",
    "plt.plot(x,y,'r',linewidth=3)\n",
    "\n",
    "y,x = np.histogram(generated_samples,num_bins)\n",
    "x = x[:-1] + (x[1] - x[0])/2 \n",
    "plt.plot(x,y,'k--',linewidth=3)\n",
    "\n",
    "plt.legend(['Original Data','Noisy Data','Generated Data'])\n",
    "plt.title('Histograms',fontsize=24)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
